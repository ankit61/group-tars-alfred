{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit ('tars-env': venv)",
   "metadata": {
    "interpreter": {
     "hash": "bac12fe466a944f533b19a5350aef698ea50a6c5fa8f0dcdfc904a7a6c3fbffd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import json\n",
    "import pandas\n",
    "import os\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return [tok.text for tok in doc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\"train\", \"valid_seen\", \"valid_unseen\", \"tests_seen\", \"tests_unseen\"]\n",
    "task_fields = [\"task_type\", \"focus_object\", \"base_object\", \"dest_object\", \"scene\"]\n",
    "stats_dict = defaultdict(lambda: [])\n",
    "\n",
    "for split in splits:\n",
    "    for task_dir in os.listdir(split):\n",
    "        task_values = task_dir.split(\"-\")\n",
    "        for trial_dir in os.listdir(\"{}/{}\".format(split, task_dir)):\n",
    "            stats_dict[\"task_id\"].append(trial_dir)\n",
    "            stats_dict[\"split\"].append(split)\n",
    "            for i, field in enumerate(task_fields):\n",
    "                stats_dict[field].append(task_values[i])\n",
    "            traj_data = json.load(\"{}/{}/traj_data.json\".format(split, trial_dir))\n",
    "            num_steps_list = []\n",
    "            num_step_tokens_list = []\n",
    "            num_task_tokens_list = []\n",
    "            for directive in traj_data[\"turk_annotations\"][\"anns\"]:\n",
    "                num_steps_list.append(len(directive[\"high_descs\"]))\n",
    "                num_step_tokens_list.append(sum([len(tokenize(desc)) for desc in directive[\"high_descs\"]]))\n",
    "                num_task_tokens_list.append(len(tokenize(directive[\"task_desc\"])))\n",
    "            stats_dict[\"num_steps\"].append(np.mean(num_steps_list))\n",
    "            stats_dict[\"steps_num_toks\"].append(np.mean(num_step_tokens_list))\n",
    "            stats_dict[\"task_num_toks\"].append(np.mean(num_task_tokens_list))\n",
    "            stats_dict[\"timesteps\"].append(len(traj_data[\"plan\"][\"low_actions\"]))\n",
    "            navigation = 0\n",
    "            interaction = 0\n",
    "            for action in traj_data[\"plan\"][\"low_actions\"]:\n",
    "                if \"mask\" in action[\"args\"].keys():\n",
    "                    interaction += 1\n",
    "                else:\n",
    "                    navigation += 1\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  }
 ]
}