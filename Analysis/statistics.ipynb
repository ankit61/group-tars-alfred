{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-19219e5251b2>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtransformers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mRobertaTokenizerFast\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpprint\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpprint\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mwordcloud\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mWordCloud\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mspacy\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtars\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0malfred\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgen\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconstants\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mOBJECTS\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizerFast\n",
    "from nltk import word_tokenize, pos_tag, ngrams\n",
    "from transformers import RobertaTokenizerFast\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "import spacy\n",
    "from tars.alfred.gen.constants import OBJECTS"
   ]
  },
  {
   "source": [
    "# Meta Data Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbpe_tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "def get_num_tokens(text):\n",
    "    return len(word_tokenize(text))\n",
    "\n",
    "def get_num_bbpe_tokens(text):\n",
    "    return len(bbpe_tokenizer(text)[\"input_ids\"])\n",
    "\n",
    "def freqs_to_df(d, sort=True):\n",
    "    return pd.DataFrame.from_dict(d, orient=\"index\", columns=[\"Count\"]).sort_values(\"Count\", ascending=False)\n",
    "\n",
    "def check_obj_in_steps(obj_toks_lower, steps, high_idx):\n",
    "    check_steps = []\n",
    "    if high_idx < len(steps):\n",
    "        check_steps.append(steps[high_idx])\n",
    "    if high_idx > 0:\n",
    "        check_steps.append(steps[high_idx - 1])\n",
    "    if high_idx < len(steps) - 1:\n",
    "        check_steps.append(steps[high_idx + 1])\n",
    "    for step in check_steps:\n",
    "        step_lower = step.lower()\n",
    "        for tok in obj_toks_lower:\n",
    "            if tok in step_lower:\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "def task_dirs(splits, data_path):\n",
    "    for split in splits:\n",
    "        task_dirs = os.listdir(f'{data_path}/{split}')\n",
    "        for task_dir in tqdm(task_dirs):\n",
    "            yield split, task_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [\"train\", \"valid_seen\", \"valid_unseen\"]\n",
    "task_fields = [\"task_type\", \"focus_object\", \"base_object\", \"dest_object\", \"scene\"]\n",
    "data_path = \"../tars/alfred/data/json_2.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build table for generating dataset statistics\n",
    "stats_dict = defaultdict(lambda: [])\n",
    "action_freqs = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "bigram_freqs = {\"task\": defaultdict(lambda: 0), \"steps\": defaultdict(lambda: 0)}\n",
    "trigram_freqs = {\"task\": defaultdict(lambda: 0), \"steps\": defaultdict(lambda: 0)}\n",
    "\n",
    "for split, task_dir in task_dirs(splits, data_path):\n",
    "    task_values = task_dir.split(\"-\")\n",
    "    for trial_dir in os.listdir(\"{}/{}/{}\".format(data_path, split, task_dir)):\n",
    "        stats_dict[\"split\"].append(split)\n",
    "        stats_dict[\"task_id\"].append(trial_dir)\n",
    "\n",
    "        for j, field in enumerate(task_fields):\n",
    "            stats_dict[field].append(task_values[j])\n",
    "\n",
    "        traj_data_file = open(\"{}/{}/{}/{}/traj_data.json\".format(data_path, split, task_dir, trial_dir))\n",
    "        traj_data = json.load(traj_data_file)\n",
    "        num_steps_list = []\n",
    "        num_step_tokens_list = []\n",
    "        num_task_tokens_list = []\n",
    "        num_step_bbpe_tokens_list = []\n",
    "        num_task_bbpe_tokens_list = []\n",
    "\n",
    "        # common_nouns_freq = defaultdict(lambda: 0)\n",
    "        for directive in traj_data[\"turk_annotations\"][\"anns\"]:\n",
    "            task_desc_toks = word_tokenize(directive[\"task_desc\"])\n",
    "            num_task_tokens_list.append(len(task_desc_toks))\n",
    "            for bigram in ngrams(task_desc_toks, 2):\n",
    "                bigram_freqs[\"task\"][bigram] += 1\n",
    "            for trigram in ngrams(task_desc_toks, 3):\n",
    "                trigram_freqs[\"task\"][trigram] += 1\n",
    "            num_steps_list.append(len(directive[\"high_descs\"]))\n",
    "            total_steps_toks = 0\n",
    "            for desc in directive[\"high_descs\"]:\n",
    "                desc_toks = word_tokenize(desc)\n",
    "                total_steps_toks += len(desc_toks)\n",
    "                for bigram in ngrams(desc_toks, 2):\n",
    "                    bigram_freqs[\"steps\"][bigram] += 1\n",
    "                for trigram in ngrams(desc_toks, 3):\n",
    "                    trigram_freqs[\"steps\"][trigram] += 1\n",
    "            num_step_tokens_list.append(total_steps_toks)\n",
    "\n",
    "            num_step_bbpe_tokens_list.append(sum([get_num_bbpe_tokens(desc) for desc in directive[\"high_descs\"]]))\n",
    "            num_task_bbpe_tokens_list.append(get_num_bbpe_tokens(directive[\"task_desc\"]))\n",
    "\n",
    "        stats_dict[\"steps\"].append(np.mean(num_steps_list))\n",
    "        stats_dict[\"total_steps_toks\"].append(np.mean(num_step_tokens_list))\n",
    "        stats_dict[\"total_steps_bbpe_toks\"].append(np.mean(num_step_bbpe_tokens_list))\n",
    "        stats_dict[\"task_toks\"].append(np.mean(num_task_tokens_list))\n",
    "        stats_dict[\"task_bbpe_toks\"].append(np.mean(num_task_bbpe_tokens_list))\n",
    "        stats_dict[\"images\"].append(len(traj_data[\"images\"]))\n",
    "        stats_dict[\"actions\"].append(len(traj_data[\"plan\"][\"low_actions\"]))\n",
    "        stats_dict[\"high_actions\"].append(len(traj_data[\"plan\"][\"high_pddl\"]))        \n",
    "        stats_dict['total_objects'].append(len(traj_data['scene']['object_poses']))\n",
    "\n",
    "        nav_count = 0\n",
    "        interact_count = 0\n",
    "        mask_cov_per_ac = 0\n",
    "        interact_step_cov = 0\n",
    "\n",
    "        for action in traj_data[\"plan\"][\"low_actions\"]:\n",
    "            action_freqs[split][action[\"api_action\"][\"action\"]] += 1\n",
    "            args = action['discrete_action']['args']\n",
    "            if \"mask\" in args:\n",
    "                obj_name = action[\"api_action\"][\"objectId\"].split(\"|\")[0]\n",
    "                obj_toks_lower = [tok.lower() for tok in re.sub('([a-z])([A-Z])', r'\\1 \\2', obj_name).split()]\n",
    "                interact_step_cov += np.mean([check_obj_in_steps(obj_toks_lower, directive[\"high_descs\"], action[\"high_idx\"]) for directive in traj_data[\"turk_annotations\"][\"anns\"]])\n",
    "                interact_count += 1\n",
    "                mask_cov_per_ac += sum([l for _, l in args['mask']])\n",
    "            else:\n",
    "                nav_count += 1\n",
    "\n",
    "        stats_dict[\"mask_coverage_per_action\"].append(mask_cov_per_ac / (300 * 300 * interact_count))\n",
    "        stats_dict[\"nav_actions\"].append(nav_count)\n",
    "        stats_dict[\"interact_actions\"].append(interact_count)\n",
    "        stats_dict[\"interact_step_cov\"].append(interact_step_cov / interact_count)\n",
    "\n",
    "stats_df = pd.DataFrame(stats_dict)\n",
    "action_df = pd.DataFrame(action_freqs)\n",
    "task_bigram_df = freqs_to_df(bigram_freqs[\"task\"])\n",
    "steps_bigram_df = freqs_to_df(bigram_freqs[\"steps\"])\n",
    "task_trigram_df = freqs_to_df(trigram_freqs[\"task\"])\n",
    "steps_trigram_df = freqs_to_df(trigram_freqs[\"steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive some additional columns\n",
    "stats_df[\"toks/step\"] = stats_df[\"total_steps_toks\"] / stats_df[\"steps\"]\n",
    "stats_df[\"bbpe_toks/step\"] = stats_df[\"total_steps_bbpe_toks\"] / stats_df[\"steps\"]\n",
    "stats_df[\"actions/step\"] = stats_df[\"actions\"] / stats_df[\"steps\"]\n",
    "stats_df[\"images/action\"] = stats_df[\"images\"] / stats_df[\"actions\"]\n",
    "stats_df[\"nav/interact\"] = stats_df[\"nav_actions\"] / stats_df[\"interact_actions\"]\n",
    "\n",
    "stats_df = stats_df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 100)\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot frequency of a categorical field. Useful for task_type and maybe objects.\n",
    "def plot_freq(col, **kwargs):\n",
    "    splits = [\"train\", \"valid_seen\", \"valid_unseen\"]\n",
    "    freq_cols = {split:stats_df[stats_df[\"split\"] == split][col].value_counts(normalize=True) for split in splits}\n",
    "    pd.concat(freq_cols, axis=1).plot.bar(xlabel=col, ylabel=\"Relative Frequency\", **kwargs)\n",
    "\n",
    "# Plot histogram of a quantitative field. \n",
    "def plot_hist(col, **kwargs):\n",
    "    splits = [\"train\", \"valid_seen\", \"valid_unseen\"]\n",
    "    axes = stats_df.hist(col, by=\"split\", sharex=True, **kwargs)\n",
    "    for ax in axes.reshape(-1):\n",
    "        ax.tick_params(axis=\"x\", which=\"both\", labelbottom=True)\n",
    "        ax.set_xlabel(col, visible=True)\n",
    "        ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_plot = action_df.div(action_df.sum(axis=0)).plot.bar(figsize=(10, 5), xlabel=\"action\", ylabel=\"Relative Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freq(\"task_type\", figsize=(8, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freq(\"focus_object\", figsize=(20, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_freq(\"dest_object\", figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(\"steps\", figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(\"total_objects\", figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(\"mask_coverage_per_action\", figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(\"interact_step_cov\", figsize=(10, 5))"
   ]
  },
  {
   "source": [
    "# KDE per split of quantitative fields\n",
    "# Inspired by https://stackoverflow.com/questions/46045750/python-distplot-with-multiple-distributions\n",
    "\n",
    "cs = [c for c in stats_df.columns if stats_df[c].dtype != 'O']\n",
    "splits = stats_df['split'].unique()\n",
    "df = stats_df[['split'] + cs].melt(['split'], var_name='cols', value_name='vals')\n",
    "num_cols = 4\n",
    "num_rows = math.ceil(len(cs) / num_cols)\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(25, 20))\n",
    "\n",
    "col = 0\n",
    "for r in range(len(axs)):\n",
    "    for c in range(len(axs[r])):\n",
    "        if col >= len(cs):\n",
    "            break\n",
    "        for s in splits:\n",
    "            sns.distplot(stats_df[stats_df['split'] == s][cs[col]], hist=False, rug=False, ax=axs[r][c])\n",
    "        col += 1\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of quantitative fields\n",
    "stats_df.groupby('split').mean().round(2)"
   ]
  },
  {
   "source": [
    "# Textual Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_trigram_df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_trigram_df[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_word_cloud(split):\n",
    "    d = defaultdict(lambda: 0)\n",
    "\n",
    "    for nouns_dict in stats_df[stats_df['split'] == split]['common_nouns_freq']:\n",
    "        for k in nouns_dict:\n",
    "            d[k] += nouns_dict[k]\n",
    "\n",
    "    wc = WordCloud(background_color=\"white\", max_words=1000)\n",
    "    wc.generate_from_frequencies(d)\n",
    "\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_word_cloud('train')\n",
    "show_word_cloud('valid_seen')\n",
    "show_word_cloud('valid_unseen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\n",
    "common_nouns = defaultdict(lambda:set())\n",
    "common_nouns_bi = defaultdict(lambda:set())\n",
    "unk_words = defaultdict(lambda:set())\n",
    "total_words = defaultdict(lambda:0)\n",
    "\n",
    "for split, task_dir in task_dirs(splits, data_path):\n",
    "    for trial_dir in os.listdir(\"{}/{}/{}\".format(data_path, split, task_dir)):\n",
    "        traj_data_file = open(\"{}/{}/{}/{}/traj_data.json\".format(data_path, split, task_dir, trial_dir))\n",
    "        traj_data = json.load(traj_data_file)\n",
    "\n",
    "        for directive in traj_data[\"turk_annotations\"][\"anns\"]:\n",
    "            words = '. '.join(directive['high_descs'] + [directive[\"task_desc\"] + '. '])\n",
    "            tokens = nlp(words.lower())\n",
    "            total_words[split] += len(tokens)\n",
    "            for i, t in enumerate(tokens):\n",
    "                if t.pos_ == 'NOUN':\n",
    "                    common_nouns[split].add(t.text)\n",
    "                    if (i < len(tokens) - 1) and tokens[i + 1].pos_ == 'NOUN':\n",
    "                        common_nouns_bi[split].add(t.text + '|||' + tokens[i + 1].text)\n",
    "                if not t.has_vector:\n",
    "                    unk_words[split].add(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all unknown words in train and validation sets. Note most words are just misspelled, which can probably be corrected\n",
    "remove_words = defaultdict(lambda: [])\n",
    "for s in unk_words:\n",
    "    for unk in unk_words[s]:\n",
    "        if unk.strip() == '':\n",
    "            remove_words[s].append(unk)\n",
    "\n",
    "for s in unk_words:\n",
    "    unk_words[s] = unk_words[s].difference(remove_words[s])\n",
    "\n",
    "pprint(dict(unk_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of unknown words\n",
    "for s in unk_words:\n",
    "    print(f'Percentage of unknown words in {s}: {len(unk_words[s]) / total_words[s]}')"
   ]
  },
  {
   "source": [
    "synonyms = {}\n",
    "for ob in tqdm(OBJECTS):\n",
    "    ob_tokens = nlp(re.sub('([a-z])([A-Z])', r'\\1 \\2', ob).lower())\n",
    "    \n",
    "    sims = {n: ob_tokens.similarity(nlp(n)) for split in common_nouns for n in common_nouns[split]}\n",
    "    if len(ob_tokens) == 2:\n",
    "        sims.update({' '.join(n.split('|||')): ob_tokens.similarity(nlp(' '.join(n.split('|||')))) for split in common_nouns for n in common_nouns_bi[split]})\n",
    "\n",
    "    sims = sorted(sims, key=lambda k: -sims[k])\n",
    "    synonyms[ob_tokens.text] = sims[:3]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 3 synonyms for every object measured by word embedding distance\n",
    "pprint(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Percentage of objects referred to in instructions compared to objects in the scene\n",
    "\n",
    "splits = [\"train\", \"valid_seen\", \"valid_unseen\"]\n",
    "task_fields = [\"task_type\", \"focus_object\", \"base_object\", \"dest_object\", \"scene\"]\n",
    "data_path = \"../tars/alfred/data/json_2.1.0\"\n",
    "\n",
    "pseudo_attention_metric = [] # list of tuples (number of objects in ste-by-step instructions, total number of objects as per object_poses)\n",
    "corresponding_files = []\n",
    "\n",
    "for split in splits:\n",
    "    try:\n",
    "        task_dirs = os.listdir(\"{}/{}\".format(data_path, split))\n",
    "        for i in range(len(task_dirs)):\n",
    "            task_dir = task_dirs[i]\n",
    "            task_values = task_dir.split(\"-\")\n",
    "            for trial_dir in os.listdir(\"{}/{}/{}\".format(data_path, split, task_dir)):\n",
    "                traj_data_file = open(\"{}/{}/{}/{}/traj_data.json\".format(data_path, split, task_dir, trial_dir))\n",
    "                traj_data = json.load(traj_data_file)\n",
    "                object_poses_json = traj_data['scene']['object_poses']\n",
    "                object_name_list = [] # object names in the scene\n",
    "                for object_pose_json in object_poses_json:\n",
    "                    objectName = object_pose_json[\"objectName\"]\n",
    "                    object_name_list.append(objectName[:objectName.find(\"_\")])\n",
    "\n",
    "                object_name_list = list(set(object_name_list)) # remove duplicates\n",
    "\n",
    "                # Get the step by step instructions\n",
    "                count = 0\n",
    "                words_already_accounted = []\n",
    "                for instructions_json in traj_data[\"turk_annotations\"]['anns']:\n",
    "                    for step_by_step_instruction in instructions_json[\"high_descs\"]:\n",
    "                        for object in object_name_list:\n",
    "                            # compare object in scene to object referred in the instructions. Do not double-count\n",
    "                            if object.lower() not in words_already_accounted and object.lower() in step_by_step_instruction:\n",
    "                                count += 1\n",
    "                                words_already_accounted.append(object.lower())\n",
    "\n",
    "                pseudo_attention_metric.append((count, len(object_name_list), count/len(object_name_list)))\n",
    "                corresponding_files.append(traj_data_file)\n",
    "                if pseudo_attention_metric[-1][-1] > 1:\n",
    "                    print(traj_data_file)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "pseudo_attention_metric = np.asarray(pseudo_attention_metric)\n",
    "percentages = pseudo_attention_metric[:, 2]\n",
    "plt.hist(percentages, bins=10)\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"%\")\n",
    "plt.title(\"Percentage of objects referred to in instructions \\n compared to objects in the scene\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "source": [
    "# Visual Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyimages import EasyImageList\n",
    "\n",
    "seg_dir = 'instance_masks'\n",
    "rgb_dir = 'high_res_images'\n",
    "depth_dir = 'depth_images'\n",
    "\n",
    "random_trajs = [\n",
    "    '../tars/alfred/data/json_2.1.0/train/pick_two_obj_and_place-SoapBottle-None-Cabinet-406/trial_T20190909_145544_288730/',\n",
    "    '../tars/alfred/data/json_2.1.0/train/pick_and_place_with_movable_recep-Spatula-Pan-CounterTop-13/trial_T20190908_194609_016883/'\n",
    "]\n",
    "\n",
    "ims_dict = {}\n",
    "for d in [rgb_dir, depth_dir, seg_dir]:\n",
    "    ims = EasyImageList([])\n",
    "    for t in random_trajs:\n",
    "        ims.ims += EasyImageList.from_folder(os.path.join(t, d))\n",
    "    ims_dict[d] = ims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample random RGB images\n",
    "ims_dict['high_res_images'].html(sample=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample random depth images\n",
    "ims_dict['depth_images'].html(sample=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample random segmentation images\n",
    "ims_dict['instance_masks'].html(sample=33)"
   ]
  }
 ]
}